{"ts":1757024168830,"role":"user","content":"Use spawn_template with vscode_extension_test to thoroughly validate the VS Code extension. Test: 1) Compilation works, 2) Key commands (lmstudio.openChat, lmstudio.createTask) execute without errors, 3) Fix any issues found, 4) Re-test until ALL verifications pass. Do NOT mark done until every test passes successfully."}
{"ts":1757024168869,"role":"system","content":"ERROR: LLM error 404: {\n    \"error\": {\n        \"message\": \"Model \\\"lmstudio-local\\\" not found. Please specify a valid model.\\n\\nYour models:\\n\\nqwen2.5-coder-1.5b\\ngoogle_gemma-3-1b-it\\ndolphin-2.9.3-mistral-7b-32k\\nqwen3coder\\nqwen3-4b-thinking-2507\\nmeta-llama-3-8b-instruct\\nvikhr-gemma-2b-instruct\\ntext-embedding-nomic-embed-text-v1.5\\ngoogle_gemma-3-4b-it\\nmistralai/mistral-small-3.2\\ngoogle_gemma-3-12b-it\\nmicrosoft/phi-4-mini-reasoning\\nmungert/glm-4.1v-9b-thinking\\nliquid/lfm2-1.2b\\nllama-3.2-3b-claude-3.7-sonnet-reasoning-distilled\\ndeepseek-r1-distill-qwen-1.5b\\ncodegemma-1.1-2b\\nphi-2\\ndeepseek-coder-1.3b-base\\nglm-4-9b-0414\\nstable-code-3b\\nyi-1.5-6b-chat\\ncodeqwen1.5-7b-chat\\nunsloth/glm-4.1v-9b-thinking\\nglm-4-32b-0414\\ntinyllama-1.1b-chat-v1.0\\ndirectml-int4-awq-block-128\\n22cb240e0292b0b5ab4c17ccd97aa3a2f799cbed\\n0060bc56d46589041c1048efd1a397421b1142b5\\n8f445e3628f3500ee69f24e1303c9f10f5342a39\",\n        \"type\": \"invalid_request_error\",\n        \"param\": \"model\",\n        \"code\": \"model_not_found\"\n    }\n}"}
